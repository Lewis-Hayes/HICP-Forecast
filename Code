# ================================================================
# Import Packages
# ================================================================

# Import all required packages in one cell
import warnings
warnings.filterwarnings('ignore')

# Core data processing and modeling
import pandas as pd
import numpy as np
import joblib

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Time series analysis
from statsmodels.tsa.stattools import adfuller, kpss
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.stats.stattools import durbin_watson, jarque_bera
from statsmodels.stats.diagnostic import het_breuschpagan, acorr_breusch_godfrey, acorr_ljungbox
from statsmodels.regression.linear_model import OLS
from statsmodels.tools.tools import add_constant
from statsmodels.tsa.arima.model import ARIMA, ARIMAResults
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Machine learning
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.linear_model import LinearRegression, QuantileRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.inspection import permutation_importance
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import BayesianRidge
from sklearn.feature_selection import RFECV
from sklearn.base import clone

# Specialized models
import lightgbm as lgb
import xgboost as xgb
import optuna
import shap
from catboost import CatBoostRegressor
from ngboost import NGBRegressor
from ngboost.distns import Normal, LogNormal

# Data acquisition
import requests
from fredapi import Fred
from tqdm import tqdm

print("All packages imported successfully!")

# ================================================================
# Data Collection
# ================================================================

# Initialize FRED API
fred = Fred(api_key='9b5242f1201443c1a076e1a6f3cc5874')

# Enhanced frequency conversion (keeps raw values)
def convert_to_monthly(series):
    if series.empty or not isinstance(series.index, pd.DatetimeIndex):
        return series
        
    # Handle different frequencies without transformations
    if len(series) > 12:  # Higher frequency data
        return series.resample('M').last().ffill()
    elif len(series) < 12:  # Quarterly/annual data
        return series.resample('MS').interpolate(method='linear')
    return series.resample('M').last().ffill()

# Read Excel data from 'Money_Supply_Data.xlsx'
excel_path = r'C:\Users\Lewis Hayes\Documents\Portfolio Project\Money_Supply_Data.xlsx'
excel_data = pd.read_excel(excel_path, sheet_name='Sheet1')
excel_data['Date'] = pd.to_datetime(excel_data['Date'])
excel_data.set_index('Date', inplace=True)

# Create configuration for Excel series (prefix keys with 'Excel_' to avoid conflicts)
excel_sources = {}
for col in excel_data.columns:
    key = f"Excel_{col}"  # Unique key to avoid overwriting FRED series
    excel_sources[key] = {
        'type': 'excel', 
        'region': 'Unknown',  # Default region (user can update later)
        'original_col': col   # Stores original column name for reference
    }

# FRED sources with raw data
reliable_sources = {
    'EA_Core_HICP': {'type': 'fred', 'code': 'TOTNRGFOODEA20MI15XM', 'region': 'Euro Area'},
    'EA_HICP': {'type': 'fred', 'code': 'CP0000EZ19M086NEST', 'region': 'Euro Area'},
    'EA_Deposit_Rate': {'type': 'fred', 'code': 'ECBDFR', 'region': 'Euro Area'},
    'EA_10Y_Bond_Yield': {'type': 'fred', 'code': 'IRLTLT01EZM156N', 'region': 'Euro Area'},
    'Baltic_Dry_Index': {'type': 'fred', 'code': 'BOGZ1FL893169105Q', 'region': 'Global'},
    'NonEnergy_Commodities': {'type': 'fred', 'code': 'PALLFNFINDEXQ', 'region': 'Global'},
    'Energy_Commodities': {'type': 'fred', 'code': 'PNRGINDEXM', 'region': 'Global'},
    'Crude_Oil': {'type': 'fred', 'code': 'DCOILBRENTEU', 'region': 'Global'},
    'Industrial_Raw_Materials': {'type': 'fred', 'code': 'PRAWMINDEXM', 'region': 'Global'},
    'Food_Beverages': {'type': 'fred', 'code': 'PFOODINDEXM', 'region': 'Global'},
    'Wheat_Prices': {'type': 'fred', 'code': 'PWHEAMTUSDM', 'region': 'Global'},
    'EUR_USD': {'type': 'fred', 'code': 'DEXUSEU', 'region': 'Euro Area'},
    'EA_Effective_Exchange_Rate': {'type': 'fred', 'code': 'RBSEBIS', 'region': 'Euro Area'},
    'EA_Euribor_3M': {'type': 'fred', 'code': 'IR3TIB01EZM156N', 'region': 'Euro Area'},
    'US_10Y_Inflation_Expect': {'type': 'fred', 'code': 'T10YIE', 'region': 'US'},
    'ECB_Balance_Sheet': {'type': 'fred', 'code': 'ECBASSETSW', 'region': 'Euro Area'},
    'EA_STOXX50_Index': {'type': 'fred', 'code': 'SPASTT01EZM661N', 'region': 'Euro Area'}
}

# Combine FRED and Excel configurations
all_sources = {**reliable_sources, **excel_sources}

# Download all series (raw data) and process Excel data
data_dict = {}
for name, config in tqdm(all_sources.items(), desc="Downloading and processing predictors"):
    if config['type'] == 'fred':
        try:
            data = fred.get_series(config['code'])
            data_dict[name] = convert_to_monthly(data)
        except Exception as e:
            print(f"FRED series {config['code']} failed: {str(e)}")
            data_dict[name] = pd.Series(dtype=float)
    elif config['type'] == 'excel':
        # Process Excel series using the original column name
        s = excel_data[config['original_col']]
        data_dict[name] = convert_to_monthly(s)

# Combine data into a DataFrame
df_raw = pd.DataFrame({k: v for k, v in data_dict.items() if not v.empty})
df_raw.index = pd.to_datetime(df_raw.index)
df_raw.index.name = 'Date'

# Create region information DataFrame
region_info = {name: config['region'] for name, config in all_sources.items()}
region_df = pd.DataFrame.from_dict(region_info, orient='index', columns=['Region'])

# Create source information DataFrame
source_list = []
identifier_list = []
for name, config in all_sources.items():
    source_list.append('FRED' if config['type'] == 'fred' else 'Excel')
    identifier_list.append(config['code'] if config['type'] == 'fred' else config['original_col'])

source_info = pd.DataFrame({
    'Source': source_list,
    'Identifier': identifier_list
}, index=all_sources.keys())

# Save to Excel
output_file = "raw_inflation_predictors.xlsx"
with pd.ExcelWriter(output_file) as writer:
    df_raw.to_excel(writer, sheet_name='Raw Data')
    region_df.to_excel(writer, sheet_name='Region Info')
    source_info.to_excel(writer, sheet_name='Source Info')

print(f"\nRaw dataset saved to {output_file}")
print(f"Time range: {df_raw.index.min()} to {df_raw.index.max()}")
print(f"Variables: {', '.join(df_raw.columns.tolist())}")

# ================================================================
# Variable Creation and Manipulation
# ================================================================

# Create your transformed variables
df_raw['HICP_YoY'] = df_raw['EA_HICP'].pct_change(periods=12) * 100
df_raw['Core_HICP_YoY'] = df_raw['EA_Core_HICP'].pct_change(periods=12) * 100
df_raw['HICP_MoM'] = df_raw['EA_HICP'].pct_change() * 100
df_raw['Core_HICP_MoM'] = df_raw['EA_Core_HICP'].pct_change() * 100

# Drop the original columns
df_raw = df_raw.drop(['EA_HICP', 'EA_Core_HICP'], axis=1)

# Optionally: Rename your new variables to match original naming convention if desired
df_raw = df_raw.rename(columns={
    'HICP_YoY': 'EA_HICP_YoY',
    'Core_HICP_YoY': 'EA_Core_HICP_YoY',
    'HICP_MoM': 'EA_HICP_MoM',
    'Core_HICP_MoM': 'EA_Core_HICP_MoM'
})

# Update the reliable_sources dictionary with your new variables
new_variables = {
    'EA_HICP_YoY': {'type': 'transformed', 'source': 'EA_HICP', 'region': 'Euro Area'},
    'EA_Core_HICP_YoY': {'type': 'transformed', 'source': 'EA_Core_HICP', 'region': 'Euro Area'},
    'EA_HICP_MoM': {'type': 'transformed', 'source': 'EA_HICP', 'region': 'Euro Area'},
    'EA_Core_HICP_MoM': {'type': 'transformed', 'source': 'EA_Core_HICP', 'region': 'Euro Area'}
}

# Update the region_info and source_info for saving to Excel
for name, config in new_variables.items():
    region_info[name] = config['region']

# Save the updated dataframe to a new Excel file
updated_file = "transformed_inflation_predictors.xlsx"
with pd.ExcelWriter(updated_file) as writer:
    df_raw.to_excel(writer, sheet_name='Transformed Data')
    
    # Update region info sheet
    region_df = pd.DataFrame.from_dict(region_info, orient='index', columns=['Region'])
    region_df.to_excel(writer, sheet_name='Region Info')
    
    # Update source info sheet
    source_info = pd.DataFrame({
        'Source': ['FRED' if col not in new_variables else 'Transformed' 
                   for col in df_raw.columns],
        'Original Series': [reliable_sources.get(col, {}).get('code', 'N/A') 
                            if col not in new_variables else new_variables[col]['source']
                            for col in df_raw.columns]
    }, index=df_raw.columns)
    source_info.to_excel(writer, sheet_name='Source Info')

print(f"Updated dataset saved to {updated_file}")
print(f"New variables: {', '.join(new_variables.keys())}")
print(f"Removed variables: EA_HICP, EA_Core_HICP")

# ================================================================
# Interpolation, Unit Root Test and Differencing
# ================================================================

def adf_test(series, alpha=0.05):
    """Perform ADF test and return whether series is stationary"""
    result = adfuller(series.dropna())
    return result[1] < alpha  # True if stationary

def kpss_test(series, alpha=0.05, regression='c'):
    """Perform KPSS test and return whether series is stationary"""
    result = kpss(series.dropna(), regression=regression)
    return result[1] > alpha  # True if stationary (KPSS has opposite null hypothesis)

def make_stationary(series, max_diffs=2, alpha=0.05):
    """Apply differencing until both tests agree on stationarity or max_diffs reached"""
    diffs = 0
    current = series.copy()
    conflict_resolution_attempted = False
    
    while diffs <= max_diffs:
        adf_result = adf_test(current, alpha)
        kpss_result = kpss_test(current, alpha)
        
        if adf_result and kpss_result:
            return current, diffs, 'Stationary'
        elif not adf_result and not kpss_result:
            current = current.diff().dropna()
            diffs += 1
            continue
        else:
            if not conflict_resolution_attempted:
                kpss_result_trend = kpss_test(current, alpha, regression='ct')
                if adf_result and kpss_result_trend:
                    return current, diffs, 'Trend-Stationary'
                conflict_resolution_attempted = True
                continue
            return current, diffs, 'Ambiguous'
    return current, diffs, 'Non-Stationary'

# Copy dataframe and prepare
df_stationary = df_raw.copy()
df_stationary.index = pd.to_datetime(df_stationary.index)
df_stationary = df_stationary.sort_index()

columns_to_interpolate = ['Baltic_Dry_Index', 'NonEnergy_Commodities']

for col in columns_to_interpolate:
    df_stationary.loc[df_stationary[col] == 0, col] = pd.NA
    df_stationary[col] = df_stationary[col].interpolate(method='linear', limit_direction='both')
    df_stationary[col] = df_stationary[col].fillna(method='bfill').fillna(method='ffill')
    print(f"{col} interpolation done. Remaining NaNs: {df_stationary[col].isna().sum()}")

stationarity_info = {}

for col in tqdm(df_stationary.columns, desc="Processing variables"):
    stationary_series, diffs_applied, status = make_stationary(df_stationary[col])
    df_stationary[col] = stationary_series
    stationarity_info[col] = {
        'diffs_applied': diffs_applied,
        'adf_stationary': adf_test(stationary_series),
        'kpss_stationary': kpss_test(stationary_series),
        'status': status,
        'final_kpss_stat': kpss(stationary_series.dropna())[0],
        'final_adf_pvalue': adfuller(stationary_series.dropna())[1]
    }

problem_cols = [col for col, info in stationarity_info.items() 
                if info['status'] in ['Non-Stationary', 'Ambiguous']]
if problem_cols:
    print(f"Removing problematic columns: {', '.join(problem_cols)}")
    df_stationary = df_stationary.drop(columns=problem_cols)

stationarity_df = pd.DataFrame.from_dict(stationarity_info, orient='index')
stationarity_df.index.name = 'Variable'

diff_summary = stationarity_df['diffs_applied'].value_counts().sort_index()
status_summary = stationarity_df['status'].value_counts()

# Export to Excel with number formatting using xlsxwriter
stationary_file = "comprehensive_stationary_dataset.xlsx"
with pd.ExcelWriter(stationary_file, engine='xlsxwriter') as writer:
    df_stationary.to_excel(writer, sheet_name='Stationary Data')
    stationarity_df.to_excel(writer, sheet_name='Stationarity Info')
    
    test_values = pd.DataFrame({
        'ADF p-value Threshold': [0.05],
        'KPSS Critical Value (5%)': [stationarity_df['final_kpss_stat'].median()]
    })
    test_values.to_excel(writer, sheet_name='Test Parameters')
    
    workbook  = writer.book
    
    # Format for numeric cells (2 decimals)
    num_format = workbook.add_format({'num_format': '0.0000'})
    
    # Format Stationary Data sheet columns
    ws1 = writer.sheets['Stationary Data']
    last_col_1 = len(df_stationary.columns) - 1
    ws1.set_column(0, last_col_1, None, num_format)
    
    # Format Stationarity Info sheet columns
    ws2 = writer.sheets['Stationarity Info']
    last_col_2 = len(stationarity_df.columns) - 1
    ws2.set_column(0, last_col_2, None, num_format)

print(f"\nFinal dataset saved to {stationary_file}")
print(f"Time range: {df_stationary.index.min()} to {df_stationary.index.max()}")
print("\nDifferencing Summary:")
print(diff_summary.to_string())
print("\nFinal Stationarity Status:")
print(status_summary.to_string())
print("\nKey to Status:")
print("- Stationary: Both ADF and KPSS agree series is stationary")
print("- Trend-Stationary: Stationary around deterministic trend")
print("- Ambiguous: Conflicting test results after resolution attempts")
print("- Non-Stationary: Failed to achieve stationarity after max differencing")

# ================================================================
# Further Variable Manipulation
# ================================================================

columns_to_interpolate = ['Baltic_Dry_Index', 'NonEnergy_Commodities']

for col in columns_to_interpolate:
    # Replace zeros with NaN
    df_stationary.loc[df_stationary[col] == 0, col] = pd.NA

    # Interpolate linearly between values
    df_stationary[col] = df_stationary[col].interpolate(method='linear', limit_direction='both')

    # Fill any NaNs at start or end
    df_stationary[col] = df_stationary[col].fillna(method='bfill').fillna(method='ffill')

    print(f"Interpolation done on {col}. Remaining NaNs: {df_stationary[col].isna().sum()}")

# Now set last cells to empty strings (will export as blank cells)
df_stationary.loc[df_stationary.index[-3:], 'NonEnergy_Commodities'] = ''
df_stationary.loc[df_stationary.index[-6:], 'Baltic_Dry_Index'] = ''
df_stationary.loc[df_stationary.index[:12], 'Baltic_Dry_Index'] = ''
df_stationary.loc[df_stationary.index[:690], 'NonEnergy_Commodities'] = ''

df_stationary = df_stationary.iloc[:-1]

# Export the updated dataframe to Excel with numeric formatting
output_file = "df_stationary_interpolated.xlsx"

with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
    df_stationary.to_excel(writer, sheet_name='Stationary Data')
    workbook = writer.book
    worksheet = writer.sheets['Stationary Data']

    # Format numeric columns with 4 decimal places
    num_format = workbook.add_format({'num_format': '0.0000'})

    # Set format for all columns except index (assumed first col)
    last_col = len(df_stationary.columns)
    worksheet.set_column(1, last_col, None, num_format)

print(f"Interpolated dataframe exported to {output_file}")

# ================================================================
# Forward Fill and Removal of Variables
# ================================================================

# Convert all interaction inputs to numeric first
numeric_cols = [
    'Energy_Commodities', 'EUR_USD', 'Excel_M1_YoY (%)',
    'EA_Deposit_Rate', 'EA_10Y_Bond_Yield', 'US_10Y_Inflation_Expect',
    'Food_Beverages', 'EA_Effective_Exchange_Rate',
    'Baltic_Dry_Index', 'Industrial_Raw_Materials', 'NonEnergy_Commodities'
]

for col in numeric_cols:
    df_stationary[col] = pd.to_numeric(df_stationary[col], errors='coerce')

# Forward fill to round out the dataframe for specific columns before interaction terms
columns_to_ffill = ['Baltic_Dry_Index', 'NonEnergy_Commodities', 'Industrial_Raw_Materials']
for col in columns_to_ffill:
    df_stationary[col] = df_stationary[col].fillna(method='ffill')

# Now create interaction terms
df_stationary['Energy_ExchRate'] = df_stationary['Energy_Commodities'] * df_stationary['EUR_USD']
df_stationary['Food_EffectiveExch'] = df_stationary['Food_Beverages'] * df_stationary['EA_Effective_Exchange_Rate']
df_stationary['Industrial_RawMaterials'] = df_stationary['Baltic_Dry_Index'] * df_stationary['Industrial_Raw_Materials']

# Verify stationarity
print("Interaction term stationarity check:")
for term in ['Energy_ExchRate', 'Food_EffectiveExch', 'Industrial_RawMaterials']:
    p_value = adfuller(df_stationary[term].dropna())[1]
    print(f"{term}: ADF p-value = {p_value:.4f} {'(Stationary)' if p_value < 0.05 else ''}")

# Drop Baltic_Dry_Index before export
df_stationary = df_stationary.drop(columns=['Baltic_Dry_Index'])
df_stationary = df_stationary.drop(columns=['NonEnergy_Commodities'])

# Export
df_stationary.to_excel('interaction_terms_stationary.xlsx', index=True)

# ================================================================
# Trimming Dataset
# ================================================================

# Create a copy starting from 2003-04-30
df_stationary_trimmed = df_stationary[df_stationary.index >= '2003-04-30 00:00:00'].copy()
df_stationary_trimmed = df_stationary_trimmed.iloc[:-1, :]

print(f"Trimmed dataframe starts from: {df_stationary_trimmed.index.min()}")
print(f"Number of rows in trimmed dataframe: {len(df_stationary_trimmed)}")

# ================================================================
# Splitting train/test set and deciding target variables
# ================================================================

final_df = df_stationary_trimmed.copy()

# Define target variable (make sure it's still in your dataframe after differencing)
target_var = 'EA_HICP_MoM'  # Adjust if your column was renamed or differenced

# All other columns as predictors
predictors = [col for col in final_df.columns if col != target_var]

# Split into train/test (80%/20%) preserving time order
train_size = int(len(final_df) * 0.8)
df_train = final_df.iloc[:train_size].copy()
df_test = final_df.iloc[train_size:].copy()

# Create feature/target sets for standard models
X_train = df_train[predictors]
y_train = df_train[target_var]

X_test = df_test[predictors]
y_test = df_test[target_var]

# For ARIMA/SARIMAX models (need target series + exogenous variables)
train_arima = df_train[target_var]
test_arima = df_test[target_var]
train_exog = df_train[predictors]
test_exog = df_test[predictors]

# Print dataset details
print(f"Train period: {df_train.index.min()} to {df_train.index.max()}")
print(f"Test period: {df_test.index.min()} to {df_test.index.max()}")
print(f"\nTarget variable: {target_var}")
print(f"Number of predictors: {len(predictors)}")
print(f"Predictor examples: {', '.join(predictors[:5])}...")

print("\nDataset contains:")
print(f"- Stationary variables: {len([col for col in predictors if col not in ['Energy_ExchRate', 'MoneySupply_DepositRate', 'YieldSpread_InflExpect', 'Food_EffectiveExch', 'Industrial_RawMaterials'] and not col.startswith('CoreHICP_MoM_lag')])}")
print(f"- Interaction terms: {len([col for col in predictors if col in ['Energy_ExchRate', 'MoneySupply_DepositRate', 'YieldSpread_InflExpect', 'Food_EffectiveExch', 'Industrial_RawMaterials']])}")
print(f"- Lagged target features: {len([col for col in predictors if col.startswith('CoreHICP_MoM_lag')])}")

# ================================================================
# Leakage‑free pipelines, Bayesian tuning, ensemble forecasting
# ================================================================

# ================================================================
# FIXED: PREVENT DATA LEAKAGE
# ================================================================

# Custom pipeline to prevent leakage
class LeakageFreePipeline(Pipeline):
    """Pipeline that prevents data leakage during cross-validation"""
    def fit(self, X, y=None):
        # Only fit transformers on training data
        Xt = X
        for name, transform in self.steps[:-1]:
            if hasattr(transform, 'fit_transform'):
                Xt = transform.fit_transform(Xt, y)
            else:
                Xt = transform.fit(Xt, y).transform(Xt)
        self.steps[-1][1].fit(Xt, y)
        return self

# ================================================================
# CUSTOM SCORER FOR QUANTILE REGRESSION
# ================================================================

def quantile_score(q):
    """Create a quantile loss scorer for specific quantile"""
    def score_func(y_true, y_pred):
        return -mean_pinball_loss(y_true, y_pred, alpha=q)
    return make_scorer(score_func, greater_is_better=True)

# Prepare data with lagged features
target_var = 'EA_HICP_MoM'
predictors = final_df.columns.drop(target_var).tolist()

# Add lagged target features (1-3 months)
for lag in [1, 2, 3]:
    final_df[f'{target_var}_lag{lag}'] = final_df[target_var].shift(lag)

# Add rolling statistics
final_df[f'{target_var}_rolling_mean_3'] = final_df[target_var].rolling(window=3).mean()
final_df[f'{target_var}_rolling_std_3'] = final_df[target_var].rolling(window=3).std()

# Update predictors
predictors = final_df.columns.drop(target_var).tolist()

X = final_df[predictors]
y = final_df[target_var]

# Train-test split (80/20) with temporal order
train_size = int(len(final_df) * 0.8)
X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]
y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]

# Check for missing values
print(f"Missing values in training set: {X_train.isnull().sum().sum()}")
print(f"Missing values in test set: {X_test.isnull().sum().sum()}")

# FIXED: Remove global imputation - will handle in pipelines
# Time-series cross-validation
tscv = TimeSeriesSplit(n_splits=3)

# FIXED: Updated evaluate_model to prevent leakage
def evaluate_model(model, params, model_name):
    print(f"\n{'='*50}")
    print(f"Training {model_name} with Bayesian Optimization")
    print(f"{'='*50}")
    
    # Create leakage-free pipeline
    pipeline = LeakageFreePipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('model', model)
    ])
    
    # Update parameter names for pipeline
    fixed_params = {}
    for param, space in params.items():
        fixed_params[f'model__{param}'] = space
    
    opt = BayesSearchCV(
        estimator=pipeline,
        search_spaces=fixed_params,
        cv=tscv,
        n_iter=15,
        n_jobs=-1,
        scoring='neg_root_mean_squared_error',
        random_state=42
    )
    
    # FIXED: Use raw data without pre-imputation
    opt.fit(X_train, y_train)
    
    print(f"Best {model_name} params:")
    for k, v in opt.best_params_.items():
        print(f"{k}: {v}")
    
    # Evaluate
    train_pred = opt.predict(X_train)
    test_pred = opt.predict(X_test)
    
    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    mae = mean_absolute_error(y_test, test_pred)
    
    print(f"\n{model_name} Performance:")
    print(f"Train RMSE: {train_rmse:.4f}")
    print(f"Test RMSE: {test_rmse:.4f}")
    print(f"Test MAE: {mae:.4f}")
    
    # Residual analysis
    residuals = y_test - test_pred
    plt.figure(figsize=(12, 6))
    plt.scatter(test_pred, residuals, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.title(f'{model_name} Residual Plot')
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{model_name}_residuals.png', dpi=300)
    plt.close()
    
    # Q-Q plot
    plt.figure(figsize=(8, 6))
    probplot(residuals, dist="norm", plot=plt)
    plt.title(f'{model_name} Q-Q Plot of Residuals')
    plt.tight_layout()
    plt.savefig(f'{model_name}_qqplot.png', dpi=300)
    plt.close()
    
    # Feature importance for tree-based models
    if hasattr(opt.best_estimator_.named_steps['model'], 'feature_importances_'):
        importances = opt.best_estimator_.named_steps['model'].feature_importances_
        sorted_idx = np.argsort(importances)[::-1]
        
        plt.figure(figsize=(12, 8))
        plt.barh(np.array(predictors)[sorted_idx][:15], importances[sorted_idx][:15])
        plt.title(f"{model_name} Feature Importance")
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig(f'{model_name}_feature_importance.png', dpi=300)
        plt.close()
    
    return opt.best_estimator_, test_rmse

# ================================================================
# 1. BAYESIAN QUANTILE REGRESSION (FIXED)
# ================================================================

print("\nTraining Bayesian Quantile Regression Models for Uncertainty Estimation")

quantiles = [0.05, 0.5, 0.95]
quantile_models = {}

for q in quantiles:
    print(f"\nTraining for quantile: {q}")
    model = xgb.XGBRegressor(
        objective='reg:quantileerror',
        quantile_alpha=q,
        random_state=42,
        missing=np.nan
    )
    
    params = {
        'n_estimators': Integer(100, 500),
        'max_depth': Integer(3, 6),
        'learning_rate': Real(0.01, 0.2),
        'subsample': Real(0.7, 1.0),
        'gamma': Real(0, 3),
    }
    
    scorer = quantile_score(q)
    
    # Create leakage-free pipeline
    pipeline = LeakageFreePipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('model', model)
    ])
    
    # Update parameter names
    fixed_params = {f'model__{k}': v for k, v in params.items()}
    
    opt = BayesSearchCV(
        estimator=pipeline,
        search_spaces=fixed_params,
        cv=tscv,
        n_iter=10,
        n_jobs=-1,
        scoring=scorer,
        random_state=42
    )
    
    # FIXED: Use raw data
    opt.fit(X_train, y_train)
    quantile_models[q] = opt.best_estimator_

# Generate predictions
quantile_preds = {}
for q in quantiles:
    quantile_preds[q] = quantile_models[q].predict(X_test)

pred_interval_upper = quantile_preds[0.95]
pred_interval_lower = quantile_preds[0.05]
median_pred = quantile_preds[0.5]

# ================================================================
# 2. FIXED FEATURE SELECTION WITH IMPORTANCE GETTER
# ===============================================================

print("\nPerforming Feature Selection...")

# Create pipeline with feature_importances_ property
class FeatureImportancePipeline(LeakageFreePipeline):
    """Pipeline that exposes feature importances for RFECV"""
    @property
    def feature_importances_(self):
        return self.named_steps['model'].feature_importances_

selector_pipeline = FeatureImportancePipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('model', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))
])

selector = RFECV(
    estimator=selector_pipeline,
    step=1,
    cv=tscv,
    scoring='neg_mean_squared_error',
    min_features_to_select=15,
    importance_getter='feature_importances_'  # Directly use the property
)

selector.fit(X_train, y_train)

# Get selected features
selected_features = X_train.columns[selector.support_]
print(f"\nSelected features ({len(selected_features)}):")
print(selected_features.tolist())

# Update datasets with selected features
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# 1. XGBoost
xgb_params = {
    'n_estimators': Integer(100, 500),
    'max_depth': Integer(3, 6),
    'learning_rate': Real(0.01, 0.2),
    'subsample': Real(0.7, 1.0),
    'colsample_bytree': Real(0.7, 1.0),
    'gamma': Real(0, 3),
    'reg_alpha': Real(0, 5),
    'reg_lambda': Real(0, 5)
}
xgb_model, xgb_rmse = evaluate_model(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42, missing=np.nan),
    xgb_params,
    "XGBoost"
)

# 2. Support Vector Regression (SVR)
svr_pipeline = LeakageFreePipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', RobustScaler()),
    ('model', SVR())
])

svr_params = {
    'model__C': Real(1, 100),
    'model__epsilon': Real(0.05, 0.5),
    'model__kernel': Categorical(['linear', 'rbf']),
    'model__gamma': Real(0.01, 1)
}
svr_model, svr_rmse = evaluate_model(
    svr_pipeline,
    svr_params,
    "SVR"
)

# 3. CatBoost
cat_params = {
    'iterations': Integer(100, 500),
    'depth': Integer(3, 6),
    'learning_rate': Real(0.01, 0.2),
    'l2_leaf_reg': Real(1, 5),
    'bagging_temperature': Real(0, 0.5)
}
cat_model, cat_rmse = evaluate_model(
    CatBoostRegressor(silent=True, random_state=42, nan_mode='Min'),
    cat_params,
    "CatBoost"
)

# 4. Neural Network (MLP)
mlp_pipeline = LeakageFreePipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler()),
    ('model', MLPRegressor(max_iter=1000, early_stopping=True, random_state=42))
])

mlp_params = {
    'model__hidden_layer_sizes': Integer(20, 100),
    'model__activation': Categorical(['relu', 'tanh']),
    'model__alpha': Real(1e-5, 1e-2, prior='log-uniform'),
    'model__learning_rate_init': Real(0.001, 0.1),
    'model__batch_size': Integer(32, 64)
}
mlp_model, mlp_rmse = evaluate_model(
    mlp_pipeline,
    mlp_params,
    "Neural Network"
)

# ================================================================
# 3. MANUAL STACKING ENSEMBLE (FIXED)
# ================================================================
print("\nTraining Manual Stacking Ensemble with Tuned Meta-Model")

base_models = [
    ('xgb', xgb_model),
    ('catboost', cat_model),
    ('svr', svr_model)
]

X_meta_train = np.full((len(X_train_selected), len(base_models)), np.nan)
X_meta_test = np.zeros((len(X_test_selected), len(base_models)))

print("Generating out-of-fold predictions...")
for model_idx, (name, model) in enumerate(base_models):
    print(f"- Processing {name}...")
    
    fold_predictions = np.zeros(len(X_train_selected))
    
    for train_idx, val_idx in tscv.split(X_train_selected):
        # FIXED: Clone entire pipeline including preprocessor
        cloned_model = clone(model)
        
        # Get fold data
        X_fold_train = X_train_selected.iloc[train_idx]
        y_fold_train = y_train.iloc[train_idx]
        X_fold_val = X_train_selected.iloc[val_idx]
        
        # Fit on fold training data
        cloned_model.fit(X_fold_train, y_fold_train)
        
        # Predict on validation fold
        fold_preds = cloned_model.predict(X_fold_val)
        fold_predictions[val_idx] = fold_preds
    
    X_meta_train[:, model_idx] = fold_predictions

print("\nTraining final base models...")
for model_idx, (name, model) in enumerate(base_models):
    print(f"- Training {name} on full training set...")
    model.fit(X_train_selected, y_train)
    X_meta_test[:, model_idx] = model.predict(X_test_selected)

# Tune Bayesian meta-model
print("\nTuning Bayesian meta-model...")
meta_params = {
    'alpha_1': Real(1e-8, 1e-4, prior='log-uniform'),
    'alpha_2': Real(1e-8, 1e-4, prior='log-uniform'),
    'lambda_1': Real(1e-8, 1e-4, prior='log-uniform'),
    'lambda_2': Real(1e-8, 1e-4, prior='log-uniform'),
}

meta_opt = BayesSearchCV(
    estimator=BayesianRidge(compute_score=True),
    search_spaces=meta_params,
    cv=tscv,
    n_iter=15,
    n_jobs=-1,
    scoring='neg_mean_squared_error',
    random_state=42
)

meta_opt.fit(X_meta_train, y_train)
meta_model = meta_opt.best_estimator_

stack_pred = meta_model.predict(X_meta_test)
stack_rmse = np.sqrt(mean_squared_error(y_test, stack_pred))

# 5. Calculate Bayesian uncertainty
print("\nCalculating Bayesian uncertainty for stacking model...")
try:
    # Calculate predictive variance
    # Variance = (1 / alpha) + X @ Sigma @ X.T
    predictive_variance = (1.0 / meta_model.alpha_) + \
                          np.sum(X_meta_test @ meta_model.sigma_ * X_meta_test, axis=1)
    
    # Calculate standard deviation
    y_pred_std = np.sqrt(predictive_variance)
    
    # Calculate 95% and 90% confidence intervals
    bayesian_upper_95 = stack_pred + 1.96 * y_pred_std
    bayesian_lower_95 = stack_pred - 1.96 * y_pred_std
    bayesian_upper_90 = stack_pred + 1.645 * y_pred_std
    bayesian_lower_90 = stack_pred - 1.645 * y_pred_std
    
    print("Bayesian uncertainty calculated successfully!")
except Exception as e:
    print(f"Could not calculate Bayesian intervals: {str(e)}")
    # Fallback to point predictions
    bayesian_upper_95, bayesian_lower_95 = stack_pred, stack_pred
    bayesian_upper_90, bayesian_lower_90 = stack_pred, stack_pred

print(f"\nStacking Ensemble Performance:")
print(f"Test RMSE: {stack_rmse:.4f}")

# ================================================================
# MODEL COMPARISON AND VISUALIZATION
# ================================================================

# Model comparison
results = pd.DataFrame({
    'Model': ['XGBoost', 'SVR', 'CatBoost', 'Neural Network', 'Stacking Ensemble'],
    'Test RMSE': [xgb_rmse, svr_rmse, cat_rmse, mlp_rmse, stack_rmse]
}).sort_values('Test RMSE')

print("\nModel Comparison:")
print(results)

# Plot comparison
plt.figure(figsize=(10, 6))
plt.barh(results['Model'], results['Test RMSE'], color='skyblue')
plt.xlabel('Test RMSE')
plt.title('Model Performance Comparison')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('model_comparison.png', dpi=300)
plt.close()  # Use close instead of show to prevent blocking

# ================================================================
# ADVANCED VISUALIZATION WITH UNCERTAINTY ESTIMATES
# ================================================================

print("Creating advanced forecast visualization...")

try:
    # Create figure with constrained layout to prevent overlap
    fig, ax = plt.subplots(figsize=(16, 9), layout='constrained')
    
    # Plot actual values
    ax.plot(y_test.index, y_test, 'ko-', label='Actual Values', linewidth=2, markersize=6, alpha=0.8)
    
    # Plot median prediction
    ax.plot(y_test.index, median_pred, 'b-', label='Quantile Median Forecast', linewidth=2, alpha=0.9)
    
    # Plot quantile uncertainty bands (90%)
    ax.fill_between(y_test.index,
                    pred_interval_lower, 
                    pred_interval_upper,
                    color='blue', alpha=0.15, label='90% Quantile Interval')
    
    # Plot stacking ensemble prediction
    ax.plot(y_test.index, stack_pred, 'r--', label='Stacking Forecast', linewidth=2.5, alpha=0.9)
    
    # Plot Bayesian uncertainty bands
    ax.fill_between(y_test.index,
                    bayesian_lower_95, 
                    bayesian_upper_95,
                    color='red', alpha=0.15, label='95% Bayesian Interval')
    
    ax.fill_between(y_test.index,
                    bayesian_lower_90, 
                    bayesian_upper_90,
                    color='green', alpha=0.15, label='90% Bayesian Interval')
    
    # Formatting
    ax.set_title('Inflation Forecast with Uncertainty Estimates', fontsize=16, pad=15)
    ax.set_xlabel('Date', fontsize=12, labelpad=10)
    ax.set_ylabel('Inflation MoM%', fontsize=12, labelpad=10)
    
    # Create a single legend for all elements
    ax.legend(loc='upper left', fontsize=10, framealpha=0.9)
    
    ax.grid(alpha=0.2)
    plt.xticks(rotation=45, ha='right')
    
    # Save first to prevent memory issues
    plt.savefig('advanced_forecast.png', dpi=300, bbox_inches='tight')
    
    # Now display the plot
    plt.show()
    print("Displayed advanced forecast visualization with Bayesian uncertainty!")
    
except Exception as e:
    print(f"Error creating forecast visualization: {str(e)}")
    
    # Attempt simplified version
    try:
        plt.figure(figsize=(12, 6))
        plt.plot(y_test.index, y_test, 'k-', label='Actual')
        plt.plot(y_test.index, stack_pred, 'r--', label='Stacking Forecast')
        plt.title('Simplified Forecast')
        plt.legend()
        plt.grid(alpha=0.3)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
        print("Displayed simplified forecast visualization")
    except:
        print("Could not display any forecast visualization")

# ================================================================
# MODEL MONITORING AND RETRAINING SYSTEM (UPDATED)
# ================================================================

print("\nSetting up Model Monitoring System")

# Configuration
INITIAL_TRAIN_SIZE = len(X_train)  # Store initial size
NEW_DATA_PER_CYCLE = int(INITIAL_TRAIN_SIZE * 0.1)  # 10% new data each cycle
TEST_SIZE = int(INITIAL_TRAIN_SIZE * 0.2)  # Consistent test size
TOTAL_CYCLES = 2

def calculate_metrics(y_true, y_pred):
    """Enhanced metric calculation with validation"""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    assert len(y_true) == len(y_pred), "Mismatched lengths in metrics calculation"
    
    with np.errstate(divide='ignore', invalid='ignore'):
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        mape = np.nanmean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8)) * 100)
    return rmse, mae, mape

performance_history = []

for cycle in range(1, TOTAL_CYCLES+1):
    print(f"\n{'='*40}")
    print(f"Retraining Cycle {cycle}/{TOTAL_CYCLES}")
    print(f"{'='*40}")
    
    # 1. Get new data with boundary checks
    start_idx = INITIAL_TRAIN_SIZE + (cycle-1)*NEW_DATA_PER_CYCLE
    end_idx = min(start_idx + NEW_DATA_PER_CYCLE, len(X))
    
    if start_idx >= len(X):
        print("Warning: No more data available for retraining")
        break
        
    X_new = X.iloc[start_idx:end_idx]
    y_new = y.iloc[start_idx:end_idx]
    
    # Data quality check
    print(f"New data period: {X_new.index[0].date()} to {X_new.index[-1].date()}")
    print(f"New data shape: {X_new.shape}, Missing values: {X_new.isnull().sum().sum()}")
    
    # 2. Update training data
    X_train = pd.concat([X_train, X_new])
    y_train = pd.concat([y_train, y_new])
    
    # 3. Prepare test set with boundary checks
    test_start = end_idx
    test_end = min(test_start + TEST_SIZE, len(X))
    
    if test_start >= len(X):
        print("Warning: Insufficient data for test set")
        break
        
    X_test_new = X.iloc[test_start:test_end]
    y_test_new = y.iloc[test_start:test_end]
    
    # 4. Retrain model with validation
    try:
        retrained_model = clone(xgb_model)
        retrained_model.fit(X_train, y_train)
        
        # Model versioning
        joblib.dump(retrained_model, f'model_cycle_{cycle}.pkl')
    except Exception as e:
        print(f"Training failed: {str(e)}")
        continue
    
    # 5. Evaluate with checks
    X_test_new = X_test_new.dropna()
    if len(X_test_new) == 0:
        print("Warning: Test set empty after dropping NA")
        continue
        
    y_test_new = y_test_new[X_test_new.index]  # Align targets
    test_pred = retrained_model.predict(X_test_new)
    
    rmse, mae, mape = calculate_metrics(y_test_new, test_pred)
    
    # 6. Store comprehensive metrics
    performance_history.append({
        'cycle': cycle,
        'train_size': len(X_train),
        'test_size': len(X_test_new),
        'rmse': rmse,
        'mae': mae,
        'mape': mape,
        'train_period': f"{X_train.index[0].date()} to {X_train.index[-1].date()}",
        'test_period': f"{X_test_new.index[0].date()} to {X_test_new.index[-1].date()}",
        'new_data_points': len(X_new)
    })
    
    # 7. Print detailed report
    print(f"\nCycle {cycle} Results:")
    print(f"- Train Period: {performance_history[-1]['train_period']}")
    print(f"- Test Period: {performance_history[-1]['test_period']}")
    print(f"- Train Size: {len(X_train)} | Test Size: {len(X_test_new)}")
    print(f"- RMSE: {rmse:.4f} | MAE: {mae:.4f} | MAPE: {mape:.2f}%")
    
    # 8. Drift detection (optional)
    try:
        from alibi_detect import KSDrift
        drift_detector = KSDrift(X_train[:-NEW_DATA_PER_CYCLE], p_val=0.05)
        drift_pred = drift_detector.predict(X_new)
        print(f"- Data Drift Detected: {drift_pred['data']['is_drift']}")
    except ImportError:
        pass

# Enhanced visualization
if performance_history:
    perf_df = pd.DataFrame(performance_history)
    
    plt.figure(figsize=(14, 6))
    plt.subplot(1, 2, 1)
    plt.plot(perf_df['cycle'], perf_df['rmse'], 'bo-', label='RMSE')
    plt.plot(perf_df['cycle'], perf_df['mae'], 'ro-', label='MAE')
    plt.xlabel('Retraining Cycle')
    plt.ylabel('Error Metric')
    plt.title('Model Performance Over Time')
    plt.legend()
    plt.grid(alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(perf_df['cycle'], perf_df['train_size'], 'go-', label='Train Size')
    plt.plot(perf_df['cycle'], perf_df['test_size'], 'mo-', label='Test Size')
    plt.xlabel('Retraining Cycle')
    plt.ylabel('Dataset Size')
    plt.title('Data Growth Over Cycles')
    plt.legend()
    plt.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('retraining_performance.png', dpi=300, bbox_inches='tight')
    plt.close()

    # Save performance log
    perf_df.to_csv('retraining_performance_log.csv', index=False)
else:
    print("Warning: No performance data recorded")

print("\nModeling pipeline completed successfully!")

# ================================================================
# FINAL VISUALIZATION WITH RETRAINING CYCLES AND FORECAST
# ================================================================

plt.figure(figsize=(16, 10))

# Use compatible style
try:
    plt.style.use('seaborn-v0_8-whitegrid')  # Clean grid background
except:
    plt.style.use('ggplot')  # Fallback

# 1. Plot Actual Values
plt.plot(y.index, y, 'ko-', label='Actual Inflation', linewidth=1.5, markersize=4, alpha=0.8)

# 2. Plot Stacking Forecast with Bayesian Uncertainty
plt.plot(y_test.index, stack_pred, 'r--', linewidth=2.5, label='Stacking Forecast')
plt.fill_between(y_test.index,
                bayesian_lower_90, 
                bayesian_upper_90,
                color='red', alpha=0.15, label='90% Bayesian Interval')

# 3. Plot Quantile Forecast
plt.plot(y_test.index, median_pred, 'b-', linewidth=2, alpha=0.7, label='Quantile Median')
plt.fill_between(y_test.index,
                pred_interval_lower, 
                pred_interval_upper,
                color='blue', alpha=0.1, label='90% Quantile Interval')

# 4. Highlight Retraining Cycles
cycle1_start = pd.to_datetime("2021-01-31")
cycle1_end = pd.to_datetime("2022-09-30")
cycle2_start = pd.to_datetime("2022-10-31")
cycle2_end = pd.to_datetime("2024-06-30")

# Shaded regions for retraining
plt.axvspan(cycle1_start, cycle1_end, facecolor='#FFDDC1', alpha=0.4)  # Light orange
plt.axvspan(cycle2_start, cycle2_end, facecolor='#C1FFC1', alpha=0.4)  # Light green

# Vertical lines to mark cycle boundaries
cycle_boundaries = [cycle1_start, cycle1_end, cycle2_start, cycle2_end]
for boundary in cycle_boundaries:
    plt.axvline(boundary, color='gray', linestyle='--', alpha=0.7)

# 5. Performance Metrics Annotation
metrics_text = (
    "Retraining Performance:\n\n"
    "Cycle 1 (2021-01 to 2022-09):\n"
    "• Test Period: 2022-10 to 2025-06 (33 months)\n"
    "• RMSE: 0.3285 | MAE: 0.2231\n\n"
    "Cycle 2 (2022-10 to 2024-06):\n"
    "• Test Period: 2024-07 to 2025-06 (12 months)\n"
    "• RMSE: 0.1158 | MAE: 0.0844"
)

plt.annotate(metrics_text, 
             xy=(pd.to_datetime("2023-07-01"), plt.ylim()[1]*0.7),
             xycoords='data',
             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='gray'),
             fontsize=10)

# 6. Labels and Legend
plt.title('Euro Area Inflation Forecast with Retraining Performance', fontsize=16, pad=15)
plt.xlabel('Date', fontsize=12)
plt.ylabel('MoM Inflation %', fontsize=12)

# Custom legend
legend_elements = [
    Line2D([0], [0], color='black', marker='o', markersize=4, label='Actual Inflation'),
    Line2D([0], [0], color='red', linestyle='--', linewidth=2.5, label='Stacking Forecast'),
    Line2D([0], [0], color='blue', linewidth=2, label='Quantile Median'),
    mpatches.Patch(facecolor='red', alpha=0.15, label='90% Bayesian Interval'),
    mpatches.Patch(facecolor='blue', alpha=0.1, label='90% Quantile Interval'),
    mpatches.Patch(facecolor='#FFDDC1', alpha=0.4, label='Retraining Cycle 1'),
    mpatches.Patch(facecolor='#C1FFC1', alpha=0.4, label='Retraining Cycle 2'),
    Line2D([0], [0], color='gray', linestyle='--', label='Cycle Boundary')
]

plt.legend(handles=legend_elements, loc='lower left', fontsize=10)

# 7. Date formatting and grid
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
plt.gca().xaxis.set_major_locator(mdates.YearLocator())
plt.gcf().autofmt_xdate()  # Rotate date labels
plt.grid(True, alpha=0.3)

# 8. Add timestamp
plt.figtext(0.95, 0.01, f"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}",
           ha='right', fontsize=8, alpha=0.7)

plt.tight_layout()
plt.savefig('final_forecast_retraining.png', dpi=300, bbox_inches='tight')
plt.show()

# ================================================================
# OPTIMIZED FEATURE ANALYSIS WITH SHAP
# ================================================================

print("\nPerforming Advanced Feature Analysis")
try:
    # 1. Configuration
    top_n = 15
    shap_sample_size = min(100, len(X_train))  # Adjust sample size based on memory
    
    # 2. Get model and features
    if hasattr(xgb_model, 'named_steps'):  # Handle pipeline
        model = xgb_model.named_steps['model']
        feature_names = xgb_model[:-1].get_feature_names_out()
    else:
        model = xgb_model
        feature_names = X_train.columns.tolist()
    
    print(f"Analyzing {len(feature_names)} features")
    
    # 3. Standard Feature Importance
    if hasattr(model, 'feature_importances_'):
        importance = model.feature_importances_
        sorted_idx = np.argsort(importance)[::-1]
        top_features = np.array(feature_names)[sorted_idx][:top_n]
        top_importance = importance[sorted_idx][:top_n]
        
        plt.figure(figsize=(12, 8))
        sns.barplot(x=top_importance, y=top_features, palette='viridis')
        plt.title("Top Feature Importances (XGBoost)", fontsize=14)
        plt.xlabel("Importance Score")
        plt.ylabel("Feature")
        plt.tight_layout()
        plt.savefig('feature_importance.png', dpi=300)
        plt.show()
        
        print("\nTop Features by Importance:")
        for i in range(len(top_features)):
            print(f"{i+1}. {top_features[i]}: {top_importance[i]:.4f}")
    
except Exception as e:
    print(f"Feature analysis failed: {str(e)}")

# ================================================================
# FINAL FORECAST OUTPUT: NEXT MONTH AND YOY MEASURE
# ================================================================

print("\n" + "="*50)
print("NEXT MONTH'S INFLATION FORECAST")
print("="*50)

# Last available inflation data
last_date = final_df.index[-1]
last_mom = final_df.loc[last_date, 'EA_HICP_MoM']
last_yoy = final_df.loc[last_date, 'EA_HICP_YoY'] if 'EA_HICP_YoY' in final_df.columns else None

# Next month date
next_month_date = last_date + pd.DateOffset(months=1)

# Prepare next input row
X_next = X.iloc[[-1]].copy()

# Update lagged features
for lag in [1, 2, 3]:
    col = f'EA_HICP_MoM_lag{lag}'
    if col in X_next.columns:
        if lag == 1:
            X_next[col] = last_mom
        else:
            prev_col = f'EA_HICP_MoM_lag{lag-1}'
            X_next[col] = X_next[prev_col].values[0]

# Update rolling mean and std features
recent_mom = []
for i in range(0, 3):
    date_i = last_date - pd.DateOffset(months=i)
    if date_i in final_df.index:
        recent_mom.append(final_df.loc[date_i, 'EA_HICP_MoM'])
if len(recent_mom) > 0:
    if 'EA_HICP_MoM_rolling_mean_3' in X_next.columns:
        X_next['EA_HICP_MoM_rolling_mean_3'] = np.mean(recent_mom)
    if 'EA_HICP_MoM_rolling_std_3' in X_next.columns:
        X_next['EA_HICP_MoM_rolling_std_3'] = np.std(recent_mom)

# Generate meta-features using our base models
meta_features_next = []
for name, model in base_models:
    # Predict using each base model
    pred = model.predict(X_next[selected_features])
    meta_features_next.append(pred[0])
    
# Convert to array format for meta-model
meta_features_next = np.array(meta_features_next).reshape(1, -1)

# Predict MoM inflation and uncertainty with Bayesian meta-model
try:
    # Get prediction and standard deviation
    next_mom_pred_mean = meta_model.predict(meta_features_next)[0]
    
    # Calculate predictive variance
    # Formula: (1 / alpha) + X @ Sigma @ X.T
    pred_var = (1.0 / meta_model.alpha_) + np.sum(meta_features_next @ meta_model.sigma_ * meta_features_next, axis=1)
    pred_std = np.sqrt(pred_var)[0]
    
    # Calculate 90% confidence intervals
    next_mom_lower = next_mom_pred_mean - 1.645 * pred_std
    next_mom_upper = next_mom_pred_mean + 1.645 * pred_std
except:
    # Fallback if uncertainty calculation fails
    next_mom_pred_mean = np.mean([model.predict(X_next[selected_features])[0] for _, model in base_models])
    pred_std = np.std([model.predict(X_next[selected_features])[0] for _, model in base_models])
    next_mom_lower = next_mom_pred_mean - 1.645 * pred_std
    next_mom_upper = next_mom_pred_mean + 1.645 * pred_std

# Calculate implied YoY inflation if possible
if last_yoy is not None:
    try:
        last_11_mom = []
        for i in range(1, 12):
            date_i = last_date - pd.DateOffset(months=i)
            if date_i in final_df.index:
                last_11_mom.append(final_df.loc[date_i, 'EA_HICP_MoM'])
        
        # If we have all 11 months
        if len(last_11_mom) == 11:
            cumulative = (1 + next_mom_pred_mean/100) * np.prod([1 + m/100 for m in last_11_mom])
            next_yoy_pred = (cumulative - 1) * 100

            cumulative_lower = (1 + next_mom_lower/100) * np.prod([1 + m/100 for m in last_11_mom])
            cumulative_upper = (1 + next_mom_upper/100) * np.prod([1 + m/100 for m in last_11_mom])
            next_yoy_lower = (cumulative_lower - 1) * 100
            next_yoy_upper = (cumulative_upper - 1) * 100
        else:
            # Handle missing months by using average for missing values
            avg_mom = np.mean(last_11_mom) if last_11_mom else 0
            filled_mom = last_11_mom + [avg_mom] * (11 - len(last_11_mom))
            cumulative = (1 + next_mom_pred_mean/100) * np.prod([1 + m/100 for m in filled_mom])
            next_yoy_pred = (cumulative - 1) * 100
            next_yoy_lower = next_yoy_pred * 0.95  # Placeholder
            next_yoy_upper = next_yoy_pred * 1.05  # Placeholder
    except Exception as e:
        print(f"Error calculating YoY: {str(e)}")
        next_yoy_pred = None
else:
    next_yoy_pred = None

# Print results
print(f"\nAs of {last_date.strftime('%B %Y')}:")
print(f"- Last MoM Inflation: {last_mom:.2f}%")
if last_yoy is not None:
    print(f"- Last YoY Inflation: {last_yoy:.2f}%")

print(f"\nForecast for {next_month_date.strftime('%B %Y')}:")
print(f"- Predicted MoM Inflation: {next_mom_pred_mean:.2f}%")
print(f"  (90% CI: {next_mom_lower:.2f}% to {next_mom_upper:.2f}%)")

if next_yoy_pred is not None:
    print(f"- Implied YoY Inflation: {next_yoy_pred:.2f}%")
    if 'next_yoy_lower' in locals():
        print(f"  (90% CI: {next_yoy_lower:.2f}% to {next_yoy_upper:.2f}%)")

print("\nKey Insights:")
print("- MoM forecast shows short-term direction")
print("- YoY forecast shows annual trajectory")
print("- Confidence intervals reflect uncertainty")

print("\n" + "="*50)
print("FORECAST INTERPRETATION")
print("="*50)
if next_mom_pred_mean > 0.3:
    print("Upward pressure on prices expected next month")
elif next_mom_pred_mean < -0.1:
    print("Potential deflationary signals detected")
else:
    print("Stable price development anticipated")

if next_yoy_pred is not None:
    if next_yoy_pred > 2.5:
        print("\nWARNING: Inflation likely above ECB target (2%)")
    elif next_yoy_pred < 1.0:
        print("\nCAUTION: Low inflation environment persists")
    else:
        print("\nInflation within ECB target range (close to 2%)")

print(f"\nNote: Forecasts based on data up to {last_date.strftime('%B %d, %Y')}")
print("="*50)

# Final Takeaway: Seems too high of a forecast but we shall see where tariffs take us.